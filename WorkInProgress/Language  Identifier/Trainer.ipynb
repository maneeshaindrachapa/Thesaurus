{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import wikipedia as wiki\n",
    "from unidecode import unidecode\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_letters = 12\n",
    "language_tags = {\n",
    "\n",
    "                'en':['actor', 'alcohol', 'cheque', 'cancer', 'chocolate', 'debate', 'hobby', 'melon', 'propaganda',\n",
    "                      'religion', 'violin', 'england', 'MediaWiki'],\n",
    "\n",
    "                'si':[\"ශ්‍රී දළදා මාළිගාව\",  \"දියවඩන නිලමේ\", \"මහනුවර\", \"මාතලේ පරිපාලන දිස්ත්‍රික්කය\", \"ලංකා දිවයින\",\"සංස්කෘතිය\",\n",
    "                      \"න්‍යෂ්ටික අවි\",\"තිරසාර සංවර්ධනය\",\"කාන්තාව හා සංවර්ධනය\",\"ආර්ථික සංවර්ධනය\",\"මුල් ළමාවියේ සංවර්ධනය\",\n",
    "                     \"ඵලදායී පාසල\",\"රසායනික කර්මාන්ත\",\"පළමුවන විමලධර්මසූරිය රජ\",\"දළදා වහන්සේ\",\"දියවඩන නිලමේ\",\"මහනුවර\",\"සමාජ වෙළඳ පොළ ආර්ථිකය\",\n",
    "                     \"නාලන්දා විශ්වවිද්‍යාලය\",\"CIA ගැන අභ්‍යන්තර සමාලෝචනය\",\"CPU උපදෙස් මට්ටමේ සමාන්තරතාවය\",\"ඔලිම්පික්\",\"ශ්‍රී ලංකාවේ 14වන පාර්ලිමේන්තුව\",\"හෘදය\",\"උසස් පෙළ දේශපාලන විද්‍යාව\",\n",
    "                     \"චීන ඉතිහාසය\",\"ශ්‍රී ලංකාවේ 9වන පාර්ලිමේන්තුව\",\"යුදෙවු ආගම\",\"කොරියාවේ ඉතිහාසය\",\"ජොසොන්\",\"අක්නාටන්\",\"ජන සන්නිවේදනය\",\n",
    "                     \"ටයිටැනික් (නෞකාව)\",\"සීගිරියේ සංස්කෘතික වටිනාකම්\",\"කුමාරතුංග කැබිනට් මණ්ඩලය\",\"චෝළ රාජවංශය\",\"ධම්‌මහදයවිභඞ්‌ගො\",\"බොරතෙල්\",\"ඇඩොල්ෆ් හිට්ලර්\",\n",
    "                     \"එක් දින ජාත්‍යන්තර ක්‍රිකට් වාර්තා ලැයිස්තුව\",\"ශ්‍රී ලංකා යුද්ධ හමුදාව\",\"ඉන්දු-යුරෝපීය භාෂා පවුල\",\"ශ්‍රී ලංකාවේ 13වන පාර්ලිමේන්තුව\",\n",
    "                     \"ගනේෂ\",\"ශ්‍රී ලංකාවේ 12වන පාර්ලිමේන්තුව\",\"ශ්‍රී ලංකාවේ 10වන පාර්ලිමේන්තුව\",\"හන්ටිංටන්ගේ රෝගය\",\"බියුටි ඇන්ඩ් ද බීස්ට් (2017 චිත්‍රපටය)\"]\n",
    "\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dictionary(tag, max_word_length):\n",
    "    wiki.set_lang(tag)\n",
    "    for topic in language_tags[tag]:\n",
    "        # Scrap html content of webpage\n",
    "        page = wiki.WikipediaPage(topic)\n",
    "        #scrap plain text from page\n",
    "        content = page.content\n",
    "        #Unidecode the content\n",
    "        content = unidecode(content)\n",
    "        lst = process(content, max_word_length)\n",
    "    print(tag+\" word count: \"+str(len(lst)))\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(page_content, max_word_length):\n",
    "    #get only word with regular expression\n",
    "    words = re.sub(r'[^a-zA-Z ]', '', page_content)\n",
    "    lower = words.lower()\n",
    "    word_list = lower.split()\n",
    "    short_words = []\n",
    "    #check the word length with given max_word_length\n",
    "    for word in word_list:\n",
    "        if len(word) <= max_word_length:\n",
    "            short_words.append(word)\n",
    "    return short_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dic_to_vector(dic, max_word_length):\n",
    "    new_list = []\n",
    "    for word in dic:\n",
    "        #output vector for one word\n",
    "        vec = ''\n",
    "        n = len(word)\n",
    "        #convert charactor to one-hot-vector\n",
    "        for i in range(n):\n",
    "            #iterate each letter in word\n",
    "            current_letter = word[i]\n",
    "            #get the charactor position in a-z order\n",
    "            ind = ord(current_letter)-97\n",
    "            #Create one hot vector\n",
    "            placeholder = (str(0)*ind) + str(1) + str(0)*(25-ind)\n",
    "            #add char vector to word vector\n",
    "            vec = vec + placeholder\n",
    "        #create all waords as 12 length\n",
    "        if n < max_word_length:\n",
    "            excess = max_word_length-n\n",
    "            vec = vec +str(0)*26*excess\n",
    "        new_list.append(vec)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output_vector(tag_index, number_of_languages):\n",
    "    #Create tags for outputs\n",
    "    out = str(0)*tag_index + str(1) + str(0)*(number_of_languages-1-tag_index)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating dictionary for en\n",
      "en word count: 5901\n",
      "generating dictionary for si\n"
     ]
    }
   ],
   "source": [
    "#vector words list\n",
    "word_data = []\n",
    "#output vectors list\n",
    "language_data = []\n",
    "#word list\n",
    "master_dic = []\n",
    "\n",
    "#to keep the language count\n",
    "count = 0\n",
    "\n",
    "for tag in language_tags.keys():\n",
    "    print('generating dictionary for ' + tag)\n",
    "    #genarate dictionary for each language\n",
    "    dic = generate_dictionary(tag, max_letters)\n",
    "    #create main dictionary with all langs words\n",
    "    for word in dic:\n",
    "        master_dic.append(word)\n",
    "    #vectorize the words in language dic\n",
    "    vct = convert_dic_to_vector(dic, max_letters)\n",
    "    #add vectorized word data in to word_data array\n",
    "    for vector in vct:\n",
    "        word_data.append(vector)\n",
    "    #create output vectors\n",
    "    output_vct = create_output_vector(count, len(language_tags))\n",
    "    #add output vector data to language_data array\n",
    "    for i in range(len(vct)):\n",
    "        language_data.append(output_vct)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "\n",
    "#create [word,word_vec,lang_vec] array and append in to arr\n",
    "for i in range(len(word_data)):\n",
    "    entry = []\n",
    "    entry.append(master_dic[i])\n",
    "    for digit in language_data[i]:\n",
    "        entry.append(float(digit))\n",
    "    for digit in word_data[i]:\n",
    "        entry.append(float(digit))\n",
    "    arr.append(entry)\n",
    "\n",
    "#create dataframe with arr\n",
    "arr = np.array(arr)\n",
    "print(len(arr[1]))\n",
    "np.save('arr.npy', arr)\n",
    "df=pd.DataFrame(arr)\n",
    "df.to_csv('data.csv')\n",
    "print(\"Done\")\n",
    "print(type(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = arr\n",
    "\n",
    "inputs = data[:, 1+len(language_tags):]\n",
    "labels = data[:, 1:1+len(language_tags)]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.15)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "network = Sequential()\n",
    "network.add(Dense(200, input_dim=26*max_letters, activation='sigmoid'))\n",
    "network.add(Dense(150, activation='sigmoid'))\n",
    "network.add(Dense(100, activation='sigmoid'))\n",
    "network.add(Dense(100, activation='sigmoid'))\n",
    "network.add(Dense(len(language_tags), activation='softmax'))\n",
    "\n",
    "network.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "tboard = TensorBoard(log_dir='./logs', write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tboard]\n",
    "\n",
    "network.fit(x_train, y_train, epochs=100, batch_size=1000, validation_data=(x_test, y_test), callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing of the model\n",
    "data = pd.read_csv('test_data_sets/en.txt', sep=\" \", header=None)\n",
    "data.columns = ['word']\n",
    "for word in data['word']:\n",
    "    netword.predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fyp)",
   "language": "python",
   "name": "fyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
